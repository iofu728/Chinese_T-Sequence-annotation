\section{Models}
In this section, we describe the models used in this paper: BI-LSTM, CRF, and Bert. 
\begin{figure*}[t]
    \begin{center}
        \includegraphics[width=\textwidth]{figures/model_struct.pdf}
    \end{center}
    \caption{Two model in this paper, BiLSTM + CRF \& Bert + CRF}
    \label{fig:model}
\end{figure*}


\subsection{Bi-LSTM}
\label{sec:lstm}

Recurrent neural networks (RNNs) are a family of neural networks that operate on sequential data. They take as input a sequence of vectors $(\mathbf{x}_1, \mathbf{x}_2, \ldots, \mathbf{x}_n)$ and return another sequence $(\mathbf{h}_1, \mathbf{h}_2, \ldots, \mathbf{h}_n)$ that represents some information about the sequence at every step in the input. Although RNNs can, in theory, learn long dependencies, in practice they fail to do so and tend to be biased towards their most recent inputs in the sequence. Long Short-term Memory Networks (LSTMs) have been designed to combat this issue by incorporating a memory-cell and have been shown to capture long-range dependencies. They do so using several gates that control the proportion of the input to give to the memory cell, and the proportion from the previous state to forget.
We use the following implementation:
\\
\begin{align*}
\mathbf{i}_{t} &= \sigma(\mathbf{W}_{xi}\mathbf{x}_{t} + \mathbf{W}_{hi}\mathbf{h}_{t-1} + \mathbf{W}_{ci}\mathbf{c}_{t-1} + \mathbf{b}_{i})\\
\mathbf{c}_{t} &= (1 - \mathbf{i}_{t})\odot\mathbf{c}_{t-1} +\\
&\qquad \mathbf{i}_{t}\odot \tanh(\mathbf{W}_{xc}\mathbf{x}_{t} + \mathbf{W}_{hc}\mathbf{h}_{t-1} + \mathbf{b}_{c})\\
\mathbf{o}_{t} &= \sigma(\mathbf{W}_{xo}\mathbf{x}_{t} + \mathbf{W}_{ho}\mathbf{h}_{t-1} + \mathbf{W}_{co}\mathbf{c}_{t} + \mathbf{b}_{o})\\
\mathbf{h}_{t} &= \mathbf{o}_{t}\odot\tanh(\mathbf{c}_{t}),
\end{align*}
where $\sigma$ is the element-wise sigmoid function, and $\odot$ is the element-wise product.

For a given sentence $(\mathbf{x}_1, \mathbf{x}_2, \ldots, \mathbf{x}_n)$ containing $n$ words, each represented as a $d$-dimensional vector, an LSTM computes a representation $\overrightarrow{\mathbf{h}_t}$ of the left context of the sentence at every word $t$. Naturally, generating a representation of the right context $\overleftarrow{\mathbf{h}_t}$ as well should add useful information. This can be achieved using a second LSTM that reads the same sequence in reverse. We will refer to the former as the forward LSTM and the latter as the backward LSTM. These are two distinct networks with different parameters. This forward and backward LSTM pair is referred to as a bidirectional LSTM.

The representation of a word using this model is obtained by concatenating its left and right context representations, $\mathbf{h}_{t} = [\overrightarrow{\mathbf{h}_{t}} ; \overleftarrow{\mathbf{h}_{t}}]$. These representations effectively include a representation of a word in context, which is useful for numerous tagging applications.

\subsection{CRF Tagging Models}
\label{sec:crf}

\begin{figure*}[t]
    \begin{center}
        \includegraphics[width=\textwidth]{figures/batch.pdf}
    \end{center}
    \caption{Train Set \& Dev Set epoch \& F1 distribute in CWS problem}
    \label{fig:batch}
\end{figure*}

\begin{figure*}[t]
    \begin{center}
        \includegraphics[width=\textwidth]{figures/batch2.pdf}
    \end{center}
    \caption{Train Set \& Dev Set epoch \& F1 distribute in NER problem}
    \label{fig:batch2}
\end{figure*}

A very simple---but surprisingly effective---tagging model is to use the $\mathbf{h}_t$'s as features to make independent tagging decisions for each output $y_t$ ~\cite{ling2015finding}. Despite this model's success in simple problems like POS tagging, its independent classification decisions are limiting when there are strong dependencies across output labels. NER is one such task, since the ``grammar'' that characterizes interpretable sequences of tags imposes several hard constraints (e.g., I-PER cannot follow B-LOC;) that would be impossible to model with independence assumptions.

Therefore, instead of modeling tagging decisions independently, we model them jointly using a conditional random field. For an input sentence
$$\mathbf{X} = (\mathbf{x}_1, \mathbf{x}_2, \ldots, \mathbf{x}_n),$$
we consider $\mathbf{P}$ to be the matrix of scores output by the bidirectional LSTM network. $\mathbf{P}$ is of size $n~\times~k$, where $k$ is the number of distinct tags, and $P_{i, j}$ corresponds to the score of the $j^{th}$ tag of the $i^{th}$ word in a sentence. For a sequence of predictions
$$\mathbf{y} = (y_1, y_2, \ldots, y_n),$$
we define its score to be
$$s(\mathbf{X}, \mathbf{y})=\sum_{i=0}^{n} A_{y_i, y_{i+1}} + \sum_{i=1}^{n} P_{i, y_i}$$
where $\mathbf{A}$ is a matrix of transition scores such that $A_{i, j}$ represents the score of a transition from the tag $i$ to tag $j$. $y_0$ and $y_n$ are the \textit{start} and \textit{end} tags of a sentence, that we add to the set of possible tags. $\mathbf{A}$ is therefore a square matrix of size $k+2$.
\\
\\
A softmax over all possible tag sequences yields a probability for the sequence $\mathbf{y}$:
$$p(\mathbf{y} | \mathbf{X}) = \frac{
	e^{s(\mathbf{X}, \mathbf{y})}
}{
	\sum_{\mathbf{\widetilde{y}} \in \mathbf{Y_X}} e^{s(\mathbf{X}, \mathbf{\widetilde{y}})}
}.$$
During training, we maximize the log-probability of the correct tag sequence:

\subsection{Bert}

\input{figures/corps.tex}
\input{figures/batch_size2.tex}
\input{figures/batchSize.tex}
\input{figures/hyper_param.tex}
\input{figures/bert.tex}
\input{figures/best.tex}


BERT is one of the key innovations in the recent progress of contextualized representation learning \cite{peters2018deep,howard2018universal,devlin2018bert}.
The idea behind the progress is that even though the word embedding layer (in a typical neural network for NLP) is trained from large-scale corpora, training a wide variety of neural architectures that encode contextual representations only from the limited supervised data on end tasks is insufficient.
Unlike ELMo \cite{peters2018deep} and ULMFiT \cite{howard2018universal} that are intended to provide additional features for a particular architecture that bears human's understanding of the end task, BERT adopts a fine-tuning approach that requires almost no specific architecture for each end task. This is desired as an intelligent agent should minimize the use of prior human knowledge in the model design. Instead, it should learn such knowledge from data. BERT has two parameter intensive settings: 


\begin{itemize}
    \item {\bf \bertbase}: L=12, H=768, A=12, Total Parameters=110M
    \item {\bf \bertlarge}: L=24, H=1024, A=16, Total Parameters=340M
\end{itemize}